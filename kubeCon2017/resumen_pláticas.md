# Día 0

- Hoy en la tarde hubo lightning talks: pláticas de 5 minutos de muchos temas diferentes.
- Codefresh: Usuarios corren su código para probarlo en diferentes environments. Usan k8s para eso (ya soy toda una iniciada; no kubernetes: k8s) y aguanta bien con las imágenes sobre google cloud y toda su infraestructura tbn en google.
- Alguien habló de cómo contribuir a k8s; una chavitita que está estudiando alguna carrera. Habló del slack, de que hay Special Interest Groups que uno puede seguir en github y en slack, de que hay que leer las design proposal de cosas para ver q quieren hacer y que pronto habrá un mentoring program de k8s para contribuir y aprender.
- Alguien sobre DBs as a service: está bueno tenerlas en containers pero es un pedo el tema de que se pierdan datos. Tonces los weyes de thecodeteam.com tienen algo para persistir los datos en algún lado y nomás matar los containers de las bases.
- Algo de machine learning. Si quiero probar cosas como Tensor Flow y entrenar mil cosas de deep learning, en vez de joderme con lo que ofrece Google (no puedo mover el basecode de lo que ofrece!) o tener un cluster fisico de compus y tener que mantenerlas, estos compas hicieron un scheduler diferente para kubernetes para que se adapte mejor a lo que necesita algo de machine learning (era un pedo levantar diferentes tasks con el scheduler normal)
- Los de ebay desarrollaron tess.IO que se basa en kubernetes para manejar sus clusters, escupen logs a elasticsearch (que es tipo solr) y a postgres :heart: y luego arman dashboards con quantum (basado en kibana) que van a abrir al público el próximo año.
- Un compa levantó un cluster de kubernetes con 3 smart watches. Cada uno corriendo AsteroidOS. Para hacerlo, copió y pego mil herramientas que ya había, jajaja. Las pantallitas se veían rojas y ya que estaban levantados los noditos se ponían azules y todos aplaudían jajaja
- Una chava habló de lo chido de la comunidad de k8s. Estuvo bueno porque no tiene carrera; era niñera y ahora es intern de DevOps en Samsung SDS (?) gracias a Ada Developers Academy, que es tipo Laboratoria y está en Seattle. Hay gente que dona su tiempo para mentorear chicas programadoras. Se llama yeni capote díaz y dijo la "t" en capote todo mexicano asi que chance sea latina.
- Un dude dio tricks del kubectl, que frank podría usar jejeje; comandos y flags no tan conocidos. Luego otro hizo algo parecido sobre el minikube.
- Telepresence: un proxy frente a tu compu tal que pueda estar como parte del cluster y los cambios a tu código se vean en vivo, sin esperar a que se buildeen imágenes, se deployen y todo.
- Alguien habló de una herramienta de templates de k8s: la gente no iniciada hace unos yamls via un CLI que los valida y modifica así configuración de las aplicaciones que deployan. En vez de tener los chorromil archivos de configuración, guardan templates que se ven modificados a partir de los yamls personalizados de la gente.
- Sacaron kube-spawn que permite hacer un cluster de más de un nodo en mi compu local (ya existe minukube que sólo permite un cluster de mentis de un nodo; estos permiten más nodos locales). EL compa habló de cosas low level de containers y sólo existe en linux.
- alguien habló de un CRI proxy y no tengo idea de qué verga dijo.
- Alguien de tesla habló de que con k8s pueden tener diferentes tipos de comunicación entre cosas que desarrollan: REST, RPC y cosas como message queues; cada tipo de comunicación para l oque es adecuado.
- Alguien de Wercker habló de cómo cuando levantas muchos micro services es un pedo, tonces hacen como acuerdos via API para que no se rompa nada, pero es un pedo tener tests. Entonces hicieron un como blueprint de servicios, que no entiendo bien qué hace peeero sonaba chido ajajaja


# Día 1

*Cosas sobre la cncf*
- La Cloud Native Computing Foundation tiene dos años. Surgió a partir de que Google donara Kubernetes, a.k.a k8s, y fue el primer proyecto aceptado por ellos.
- El año pasado tenían 4 proyectos; hoy, 14. Hay más sponsors (dieron 103 diversity scholarships versus 30 que tenían pensadas porque google aflojó más lana).
- El número de gente en las conferencias ha explotado cabrón. Aquí hay 4K gentes que es más que en las 3 conferencias anteriores juntas. Próximamente hay en europa y china (para que frank mande plática :P)
- Hay un curso en edx y certificación de k8s.
- El wey de la Alibaba Cloud usa k8s.
- Hablaron de los 14 proyectos que tienen en la CNCF. Hay unos de cosas muy de kubernetes (algo de dns, containerd, rkt,cni), cosas de logs (prometheus, fluentd,opentracing,jaeger), cosas de comunicación entre servicios (linkerd, conduit, envoy, gRPC) y cosas de seguridad (TUF, notary)
- Un dude de intel habló de katacontainers, unos contenedores optimizados en hardware que son seguros y no sé qué más.

* Una chava de Netflix habló de culture y tecnología. Habló de Spinnaker (una madre para continous delivery en netflix) que se desarrolló a partir de una necesidad interna y tiene cosas muy de cultura de netflix (freedom and responsibility): te deja hacer deploys a todas las regiones a la vez pero es tu responsabilidad si lo haces; te da dashboards con info para que veas qué no funciona y no decidas a lo pendejo. Ahí cultura => tecnología pero, por otro lado, hay que ver si la tecnología que eliges refuerza tu cultura empresarial o la frena. 

* Próximamente, K8s en AWS <3. Y bien hecho: pullean directo del código de k8s, no van a hacer la mamada de tener su propia versión basada en la cosa open source. Eso va a implicar que cambien su modo de cobrar (vía algo que aún no está que se llamará Fargate): ya cobran por compu y por lambda functons; ahora hay que cobrar x container. Saldría más barato para Arquímedes que nunca tiene evaluaciones de modelos. Va a permitir todo el desmadre de aws: tener roles en los pods, tener clusters en diferentes availability zones, auth con las llaves del IAM de AWS...

* TUF y Notary: Seguridad en containers. Cuando updateo software (tipo paquetes de python o tipo versiones nuevas de software para mis cosas deployadas), cómo sé que tengo la versión más actualizada y que no está comprometido lo que ejecuto? Este show es un desmadre de llaves para firmar las actualizaciones y saber que lo que corres es legal. Tiene diferentes niveles de llaves y hay todo un pipeline para firmar cosas. Existe en docker enterprise. Puedes firmar con tus propias llaves o el coso genera cosas con RSA y otro algoritmo de encripción. Es sencillo porque hace todo por ti y te avisa si el codigo que bajaste no tiene una llave chida.

* Una chava de Twitter con una plática vergas sobre una herramienta interna para poder ver cuántos recursos ocupan los equipos internamente. Todo vía logs. Dos modelos de entidades: resources y clientes. 1) Un resource tiene tags y pertenece a un proyecto; ej: un core de un CPU que le rento a amazon que uso para evaluar el modelo de chillis. A ese recurso se le calcula un costo unitario (cuánto le pago a amazon más qué tan eficientemente lo uso para incentivar uso eficiente de cosas). En Twitter, los ejemplos eran recursos que un equipo producía y otro consumía (tipo llamadas al API). Y luego hay un pipeline de logs que saca costos y hace reportes para que cada área sepa q tan eficiente es. Tuvieron pedos con la calidad de datos: qué pasa si hay días sin logs? 2) Luego vieron que a veces la gente se mueve de equipos tonces hicieron un catálogo de gente con su modelo de entidades muy bonito. El chiste es que todo es función del tiempo: qué recursos usaba DS en enero? quién llevaba olive garden en 2016? Hay interfaz gráfica para que la gente dé de alta sus proyectos y peticiones de recursos, para cambiar metadatos de cosas, para levantar alarmas de uso raro de recursos, para dar de baja recursos, para dar de baja clientes.... Fue muy bonito, nos gustan los modelos de datos. Fue mandato de los jefes que todos usaran esa herramienta en Twitter. Tonces, cada que hay proyectos nuevos, ellos van con el equipo para garantizar que sus logs se generen y vayan a dar a cierto lado y para ayudarles a usar la herramienta interna tal que den de alta el proyecto en la aplicación y vean los reportes que genera.

* Algo sobre documentación de cosas (particularmente cosas open source). Diferentes niveles y pasos en el tiempo. Los primeros cambian menos porque son más generales. Los últimos pasos cambian más y hay que automatizar la generación de snippets y cosas que pongas en tu sitio web. Tonces, pasos: 1) en mi landing page, qué es este pedo? why should i care? 2) Getting started: no haces que bajen una máquina virtual de 10GB! tienen que ver los usuarios qué hace tu cosa. 3) Docs más largos para responder cómo solucionas un problema. Se sugieren code snippets en vez de kilómetros de explicación. 4) Qué más hace mi proyecto? Problemas particulares, mostrar lo que la comunidad hace con tu proycto. 5) Documentación más específica y larguísima que conviene automatizar. 0) Y el README.md? Para decir metas de tu proyecto, cómo instalar, cómo correrlo, en qué status está tu proyecto, quién lo mantiene, cómo contribuir, code of conduct, license. LA documentación: instructivos de legos!!! 

* Patrones de microservicios con alguien de NGINX. Hay el modelo de proxy en el que nginx cacha todo el trafico y no interviene; todos los servicios se conectan entre sí. Hay router mesh donde las peticiones les llegan a un hginx que manda a otro que se conecta directo con todos los servicios y cosas; no se conectan entre sí los servicios, es un snowflake y el nginx es el centro. Y el service mesh: cada uno de mis servicios tiene un nginx como "side car" (?); o sea, levanto mis pods con un nginx adentro. Estuvo ruda la plática. 

* Netflix y cómo solucionar pedos de permisos de los servicios o personas de hacer cosas. Open Policy Agent: una cosa en go que permite escribir policies pensadas como "este Identity puede/nopuede hacer esta Operación sobre este Recurso" y tiene un como Totoro donde la gente arma sus policies y luego las testea!!! Escribes tests sobre tus policies y se corren antes de que deployes tus policies. Entonces, cada servicio tiene un AuthAgent, que es el que se pelea con una base de datos de policies que se generaron con el Open Policy Agent.

* Intro a Prometheus. Esa madre hace que las cosas escupan logs y luego se vean en dashboards. Pésima plática peeero muy chido repo: https://github.com/errordeveloper/prom-nodejs-demo/tree/v0-start . Cada branch es un paso más que hace cambios a código y configuración hasta que ya tienes una app en Node y un prometheus y un dashboard con logs.

* Ben Siegelman y Open Tracing. Existe un estándar de tracing. No es sólo loggear; es saber qué entró y qué salió de cada servicio. Tipo los logs en base de datos de Arquimedes que guardan qué entro y que salió de cada endpoint. Eso es mejor que nomás los logs que dicen qué pasó pero no dicen qué petición ocasionó ese log. Para poder tener esto, instalas cositos en tus pods a modo de "side cars"  que escupen estos logs a algún lado. Jaeger es un cosito que hace dashboards chidos con logs. Estaba hablando de que en el mundo ideal, tener chingos de microservicios es como una parvada de pájaros que se mueve junta y es bonito y salió de la nada una como paloma en el escenario volando; fue cagado porque la siguiente diapsitiva fue que esa versión ideal es falsa y los microservices se portan mal (una foto de pájaros atacando a alguien).

* Metaparticle y Brendan Burns: están desarrollando una abstracción que permita hacer containers y pushear a clusters con comandos en el mismo lenguaje en vez de todo el cagadero que es hoy. Por ejemplo, annotations en python o javascript o lo que sea que transformen el código en un container y que lo pusheen a otro lado. Está cabrón y están en proceso. 
- Otro compa habló de un bot que le pone colores a fotos en blanco y negro en twitter que funciona gracias a Open Functions as a Service (OpenFaaS) y que desarolló un vato de 17 años. cómo? conectando cositas! Hay que desarrollar pensando en eso, en legos. 





# Día 2

* Kelsey Hightower: Update de cosas q se pueden hacer en k8s. Los features nuevos son aburridos sobre k8s! Hay que llegar a ese punto en toda tecnología porque significa que eres estabe y feliz. Levantó un cluster de k8s en la nube de Google desde el teléfono con un "OK Google bla bla". El cluster ya tenía código corriendo; hizo cambios y los deployó al cluster. El pipeline de continous delivery se ve como sigue: hago push al github en mi branch, mergeo a master, taggeo y eso triggerea un build y deploy a un ambiente de QA. La configuración para el deploy de cosas vive en otro repo (como nosotros con modelos_ds vs. opi-infraestructura). Tienes un dashboard de métricas (Istio) tbn. Con el build se actualizó el tag de versión en el repo de infraestructura para que apunte a la versión nueva. Ya que todo está bien , deploy a producción. Es importante no rebuildear! Si la imagen que probaste en QA funciona, no rebuildees!!! Qué tal que hay errores o cosas? para qué sirvió probarla si vas a deployar otra cosa. Cosas de performance o diferencias entre staging y prodcción no viven en el docker de tu contenedor! Las mandas a otros archivos para que tenga sentido hacer eso.

* Chen Goldberg, que trabaja en el equipo dentro de Google que aporta cosas al k8s open source. Antes de que k8s saliera, tenían un proyecto interno como pre-k8s para hacer más productivos a los desarrolladores pero no era flexible (cada que querían usar software diferente era un pedo) y era una blackbox para la gente (estaba bien hasta que se rompía y los desarrolladores no sabía por qué había fallado el pre-k8s que usaban). Ahora, k8s es extensible! Los features en la base de k8s son aburridos pero las cosas para hacerlo extensible siguen cambiando y las extensiones sobre el tbn. 
 - Parte de esta extensibilidad es el kube-metacontroller que permite extender k8s base sin pegarle al código core: defines tu propio cosito (StatefulSet, por ejemplo, pero custom) y lo utilizas sin pegarle al codigo base de k8s. 

* Clayton Coleman de redhat. Tienen un OpenShift Online Starter que es gratis para que la gente pueda levantar clusters de k8s y jugar. Pedos al proveer de tantos clusters a usuarios les ayudaron a mejorar cosas en k8s: tenían chingos de eventos y logs que no eran necesarios y lo corrigieron y tbn tenían chingos de archivos de los secrets (llaves, passwords) de la gente repetidos y pesaban un chingo y tbn lo corrigieron y lo aportaron al k8s base.

* HBO vivía como nosotros con ELBs y EC2s donde vivían servicios en APIs en NodeJS. Los pedos con eso eran que en las compusitas no se usaba la mitad del CPU y el autoescalamiento es tardado. Probaron Mesos, swarm y eligieron k8s. Pero hacerlo en aws era un pedo!!! 
- Existe terraform que permite tener templates para tus masters y tus mijos. Pero los pods de Prometheus se morían y tardaban en revivir. Además, amazon tardaba en darles compus que necesitaban que se solucionó creando tres tipos de mijitos: los mijos normales, los de backup y los reservados. Si un mijo normal moría, entaba un backup. Si necesitaban escalar, entraban los reservados y se iban creando nuevos. 
- Usaron Flannel para tunnear cosas de redes con los ELBs; diferentes configuraciones con ELBs.
- Usaron kube-dns. En la config le dice al cluster dónde buscar las IPS de los mijitos. Tunnearon cosas para guardar IPs en caché y no tener que buscar en todos lados y tunnearon para que las peticiones inválidas al DNS no se hicieran; que no se buscara en la lista de IPs y cosas.

* Zero trust K8s networks. Con estos desmadres se necesita que todo sea seguro: que nadie ataque mis pods y que nadie ataque la red. Istio soluciona el primero y el K8s network policy soluciona el segundo. HIbieron Calico para integrar K8s e Istio y manejarlo tal que definas un calico policy y eso implique policies en k8s y el istio y todo sea feliz porque sólo definiste cosas en un lado. Existe Tigera, una compañía que hace seguridad de k8s como para empresas, con cosas muy piquis de cada empresa. 

* K8s rode ahead. Tres ideas importantes
- Developer productivity matters. Operations tiene que funcionar cabrón para que esos weyes sigan haciendo lo suyo (tipo DS vs nosotros para deployar sus modelos).
- Multicloud. Necesitamos que se puedan levantar cosas en diferentes nubes sin que me importe en cuál y que haya estándares. La CNCF certifica cuando alguna implementación de k8s está bien hecha o no. No sólo basta con que el cluster levante en cualquier nube sino que los temas de governance tbn estén integrados; que yo ponga configuración en un lado y entonces tenga os mismos settings en todas las nubes y el mismo comportamiento esperado.
- Aplicar estas cosas en empresas es complicado por lo que tienen que pensar en la etensibilidad de k8s. Ningún único vendor va a solucionar todos los pedos de la empresa; tonces, k8s tiene que conectarse bien con todo.
Hay una paradoja de no sé quién: si haces algo más eficiente, se espera que la gente consuma menos de lo que lo alimenta pero no! cuando hubo máquinas eficientes, en vez de que se redujera el uso del carbón, aumentó cabrón. Así tiene que ser acá. Hay que solucionar esos tres pedos para que k8s jale cabrón.

* Hubo una sesión como de mentoring donde le preguntabas cosas a gente y luego un lunch con temas de diversidad. Estuvo interesante porque todos tienen los mismos pedos, no importa en dónde estemos: contratar, comunicarse entre equipos, procesos... Todo el mundo está en lo mismo. Hablé con un wey en intel que hace cosas de scheduling en k8s y ahora está en el equipo de IA en intel para que cosas de machine learning estén bien scheduleadas sobre hardware custom de intel. Tbn conocí a un tipo mamón que aporta a k8s y una chava trans que trabaja en Word Press que está migrando a k8s y empezaron por el API y usan Cassandra para guardar cosas (tipo imágenes procesadas o así porque cassandra es rapidísima y sale mejor que volver a procesar imágenes).

* Machine learning con kubernetes. Hablaron de cosas que hace kubeflow, una cosa sobre k8s, para que hacer machine learning sea más fácil. Permite orquestar deploys a diferentes ambientes sin cambios en código; incluso de mi compu hacia GPUs. Permite priorizar los recursos tal que el intern no mate el trabajo del senior DS. Tiene jupyter, tensor flow. Deja trainear y validar los modelos. Hicieron un demo que incluye una GUI para crear un manifest que pide trabajos a un cluster de k8s. 

* Principios y algunas buenas prácticas de k8s para desarrolladores. Observabilidad: k8s ve tu app y tú le setteas límites de uso para que los pods escalen horizontalmente; además, tu ves tus logs para tunnear las condiciones de escalamiento de los pods. Crash first principle: en vez de tener un chingo de ifs que controlen los reintentos de hacer cosas dentro de tu app (tipo conectarte a una DB), que se muera el pod (exit status 1), mande logs y k8s lo reviva. Unordered is better than ordered: es mejor no depender del orden pero si hay que hacerlo hay opciones; x ej, matar pods con versión vieja de recursos y revivir con versión nueva via algo que pones en el código o tener "init containers" que hagan cosas antes de tu app (algo que se enchufe a la DB antes de que se levante la app completa). Loose coupling: que los pods no se hablen entre sí; para eso hay servicios porque los nombres de las pods cambian (tipo aws que asigna ips al azar) y se tiene que acceder a recursos vía sus labels. Tight coupling: las cosas que dependan unas de otras van en un mismo pod, como el coso que manda logs (esos otros pods son los mentados side cars). Manage configuration: usar helm (una heramienta) para que tus manifestos queden en el github en un solo lugar y cuando se updateen versiones de software, se cambien cosas en el manifesto y se pusheen al git. Ask for the least amount of resources, tal que k8s pueda asignar recursos mejor (no pidas chingos de disco y chingos de ram y chingos de todo siempre; una apo puede necesitar más disco pero los logs más memoria y otra mádre más cpu). Do not reinvent the wheel: echarle ojo a los muchos proyectos alrededor de k8s como fluentd, helm, traefik, etc. 

* MySQL on K8s. Una DB es como una mascota versus un pod que es como cerdo de matadero :( . Para solucionar eso inventaron los StatefulSets que permiten que 1) no se asignen ips a lo pendejo a los pods, 2) que las cosas levanten y mueran en orden y 3) Stable storage (?). Para poder enchufar crear y configurar cosas con K8s, hay operators. Existe un mysql operator que permite usar comandos del kubeclt para prender y apagar pods con mysql que tengan backup a persisten storage (prende otro pod de backup). Hablaron de Vitess, una cosa basa en mysql pero mega shardeada que usan youtube y slack, entre otros. Igual que otras dbs distribuidas, el Vitess tiene un server donde guarda info de los shards para saber q hay en cada lugar y hay mecanismos para elegir un nuevo master si es que el original se muere. 

* Accelerating humanitarian relief con k8s. Unos compas de microsoft tienen un Project Fortis (github.com/catalystcode/project-fortis-pipeline) que hace lo que yo tenía para tesis de maestría: un pipeline de datos de diferentes fuentes, procesados y geolocalizados y dumpeados a diferentes lugares para análisis. Un dato que entra tarda 15 segundos en verse en dashboard (incluye speech recognition para audio tomado de transmisiones por radio y cosas como geolocalización de cosas con lat y lon!) Usan spark streaming para meter datos y lucene para hacer topic extraction. De ahí, mandan info por tile (tipo tile de mapbox) a cassandra para que se agregue; no usan jerarquías geopolíticas sino tiles de mapas. Existe en proyecto spark-native para enchufar k8s y spark en el que k8s se usa en lugar del zookeeper. Compilas jars que son los que se mandan a los pods para trabajos. En k8s se levantan dos namespaces al menos: el de spark y el de cassandra. Cassandra tiene que ser highly available así que se replica y todo. Eso está chido porque puedo tener datos de mi proyecto sobre zonas difíciles e inaccesibles replicados por todo el mundo. 





# Día 3

* Sarah Novotnry sobre qué pedo con la comunidad de k8s. HAce 2 años hubo 400 personas en el kubecon vs 4K ahora! Analogía: tenemos un camión al que le estamos haciendo alitas de cartón pero alguien le pegó un cochete. Sin embargo, todos los principios se intentan mantener (tienen un comité para decidir cosas)

* Alguien de github sobre su uso de k8s. Cuando migraron cosas a su propio data center dijeron "tenemos tantito tiempo libre, q hacemos?" y pensaron en algo como k8s pero cuando lo rebotaron con otros equipos internos les dijeron: están pendejos, eso va a tomar siglos. Así que no lo pudieron hacer pero ahora usan k8s para el 20% de sus servicios, incluyendo lo que powerea la web interface. Esa cosa está en rails y usan un nginx. Sus deploys incluyen un chatbot para ver el status de cosas. También incluyen levantar un ambiente para code review. Usan "canary deploys" que es taggear cierto número de pods con la tag "canary" tal que se usen como ejemplo para ver si en producción el deploy nuevo no volvió todo más lento (no es un tema de errores de código sino de performance en vivo: chance cambiaron de librería y la nueva tarda más en cargar). Si en las canary pods todo jala, se libera el deploy y conforme va pasando el tiempo todos van teniendo la nueva version. Próximamente van a partir la app de ruby en cachitos y quizá open sourceen cosas. 

* Brandon Phillips de CoreOS presentó un dashboard, techtonic, que permite ver recursos por apps. Ya tienes tus desmadres de k8s para muchas apps pero no tienes dónde ver todo lo que usas. Para eso sirve su dashboard: para ver qué tienes deployado y cómo se comporta. 

* Clayton algo que habló el día anterior sobre hacer k8s aburrido, ahora habló sobre cosas que existen alrededor de k8s para diferentes temas. Habló de que Rails partió madres porque hizo todo más facil y que k8s busca llegar al mismo punto. Dio un repaso de muchas herramientas sobre k8s para cosas de red, de observabilidad, etc. etc.

* Brian Grant. Mencionó Board o algo así que parece ser el proyecto interno de google que era k8s antes de que se abriera pero no estoy segura. Qué es k8s? Muchos modos de verlo. 1) Una plataforma de contenedores. 2) API declarativa de cosas (la línea de comandos que es el kubectl). 3) Un configuration distribution system (tipo el zookeeper). 4) Container infrastructure as a service: k8s tiene primitives que me permiten armar mi aplicación según la topología que tenga, tipo legos. 5) Platform para automatizar management de aplicaciones. 6) Services as a platform: k8s tiene una lista de servicios que puedes usar (open service broker) tipo etls, conexiones a bases de datos, mandar mails... 7) Portable cloud abstraction: k8s es portable porque los cloud providers se certifican con la CNCF para que garanticen que su version de k8s sea buena; así ya no te preocupa con cuál cloud trabajes. Además, se puede usar para diferentes tipos de workloads, incluso bases de datos con los StatefulSets. 8) Family of proyects, toolkit y ecosistema.

* Big data pipelines. Alguien presentó github.com/nuclio/nuclio. Dio un ejemplo de datos que llegan geolocalizados y se pintan en un heatmap. Levantó un spark, un zeppelin y su coso para lograrlo. Con Helm administras todas las configuraciones de todo y levantas con un comando, en vez de hacer mil comandos y tener mil archivos. Las cosas de configuración van en un ConfigMap, no en archivos dentro del pod. 

* Kafka en K8s. Aquí tbn se usan los StatefulSets para replicar la estructura de master y workers. Dio un ejemplo de pipeline en kafka donde creó un producer y un consumer de kafka también vía Helm. Los datos de kafka se distribuyen en diferentes compus (sí, pods dentro de diferentes compus) gracias a un pedo de antiaffinity que tiene el k8s. Para que no se pierdan datos, el cluster de kafka levanta con redundancia, igual que si se levanta solo. Es importante que el producer sólo escriba su stream de datos en el topic que le toca (concepto de Kafka); los permisos y todo se asignan mediante un Operator, igual que en otra platica pasada alguien dijo. Estos compas hicieron un operator open source. Tiene pedos de seguridad y tuvieron pedos cuando kafka subió de versión.

* Distributed databases en k8s, en particular Apache Ignite. El Ignite maneja la comunicación dentro de sus nodos para que no se pierdan datos y bla bla, pero k8s ayuda mediante los StatefulSets a que cada nodo nuevo que entra al cluster sepa con quién debe conectarse. Hizo un demo donde levantó un cluster de k8s con ignite adentro y vimos cómo cada nodo nuevo podía conectarse al cluster gracias a k8s. 

* Tensorflow en k8s en microsoft. Un ingeniero de infraestructura y una chava DS platicaron sobre lo que hicieron para que los tensor flows de ella jalaran chido en k8s. Hay dos tipos de paralelización en DS: o corres a la vez entrenamientos completos con sets de parámetros diferentes o paralelizas cachitos dentro de un entrenamiento dado, mueves parámetros y vuelves a probar. El de ella es el segundo tipo. Tienen unos GPUs que querían usar con K8s. Él pensó que con echarle más GPUs funcionaría pero no! Ella le explicó qué implicaba paralelizar en su mundo y él pudo organizar mejor el k8s (github.com/joyq-github/TensorFlowonK8s) tal que los GPUs se usaran al 100%. Todo esto porque el spark es disk intensive pero en cosas como tensor flow, lo que te da en la madre es la latencia de red. Hay papers de deep gradient compression (?). Microsoft sacó una cosa open source que da un dashboard para correr entrenamientos, incluyendo jupyter notebooks y ver los jobs y todo que le mando al K8s con o sin GPUs.

* 



